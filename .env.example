# ClariGen Environment Configuration
# Copy this file to .env and update with your settings

# =============================================================================
# Model Server Configuration
# =============================================================================

# Small Model (8B) - Used for fast binary ambiguity detection
# If running locally via serve_models.sh, use http://localhost:8368/v1
SMALL_MODEL_URL=http://localhost:8368/v1
SMALL_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# Large Model (70B) - Used for clarification generation and reasoning
# If running locally via serve_models.sh, use http://localhost:8369/v1
LARGE_MODEL_URL=http://localhost:8369/v1
LARGE_MODEL_NAME=nvidia/Llama-3.3-70B-Instruct-FP8

# vLLM API Key (must match the --api-key flag in your serving script)
VLLM_API_KEY=token-abc123

# =============================================================================
# Application Configuration
# =============================================================================

# API URL (used by frontend to connect to backend)
# Local: http://localhost:8370/v1
# Docker: http://api:8370/v1
API_URL=http://localhost:8370/v1

# =============================================================================
# Pipeline Configuration
# =============================================================================

# Maximum number of clarification attempts before giving up
MAX_CLARIFICATION_ATTEMPTS=3

# Clarification strategy: "at_standard", "at_cot", or "vanilla"
# - at_standard: Standard ambiguity type-based prompting (Recommended)
# - at_cot: Chain-of-thought reasoning (Better but slower)
# - vanilla: Simple prompting without extra logic
CLARIFICATION_STRATEGY=at_standard

# =============================================================================
# Logging Configuration
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path (optional, leave empty to log to console only)
LOG_FILE=

# =============================================================================
# Docker-Specific Configuration (SSH Tunnel)
# =============================================================================
# These are typically used when the models are on a remote server accessible 
# only via the SSH tunnel container.

# Example if models are tunnelled through the "tunnel" container:
# SMALL_MODEL_URL=http://tunnel:8368/v1
# LARGE_MODEL_URL=http://tunnel:8369/v1
